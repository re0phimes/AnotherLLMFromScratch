# GPT-125M 模型配置文件 - 完整示例
# 这个配置定义了一个约125M参数的GPT模型，适合在单张消费级GPU上训练和推理
# 本文件作为配置示例，包含所有参数的详细注释说明

# 基础配置示例（最小配置）:
# model_type: "gpt"
# model_name: "gpt_125m"
# n_layer: 12
# n_head: 12
# n_embd: 768
# vocab_size: 50257
# block_size: 1024

# =============================================================================
# 基本模型信息
# =============================================================================
model_type: "gpt"                    # 模型类型标识符
model_name: "gpt_125m"               # 模型名称，用于日志和保存

# =============================================================================
# 核心架构参数 - 这些参数决定了模型的基本结构和参数量
# =============================================================================

n_layer: 12                          # Transformer层数
# 说明: 控制模型深度，更多层意味着更强的表达能力但训练更慢
# 影响: 12层是GPT-2 small的标准配置，在性能和效率间取得平衡
# 参数贡献: 每层约10M参数，12层共约120M参数

n_head: 12                           # 多头注意力的头数  
# 说明: 允许模型同时关注不同类型的信息模式
# 影响: 更多头能捕获更复杂的依赖关系，但计算开销增加
# 约束: 必须能被n_embd整除，这里768/12=64，每个头64维

n_embd: 768                          # 嵌入维度（隐藏状态维度）
# 说明: 模型的"宽度"，决定了每个token的表示向量大小
# 影响: 更大维度提供更丰富的表示能力，但显存和计算需求增加
# 参数贡献: 这是参数量的主要决定因素

vocab_size: 50257                    # 词汇表大小
# 说明: 模型能理解的不同token数量，决定输入输出层大小
# 影响: 50257是GPT-2标准词汇表，包含BPE编码的所有token
# 参数贡献: 输入嵌入层 50257×768 ≈ 38M参数

block_size: 1024                     # 最大序列长度（上下文窗口）
# 说明: 模型一次能处理的最大token数量
# 影响: 更长序列能处理更长文档，但显存需求平方级增长
# 内存影响: 注意力矩阵大小为 1024×1024，需要约4MB显存(fp32)

# =============================================================================
# 模型架构细节配置
# =============================================================================

bias: true                           # 线性层是否使用偏置项
# 说明: 控制 y = Wx + b 中的偏置项b是否存在
# true:  标准配置，模型表达能力更强，参数稍多
# false: 无偏置，参数更少，某些情况下训练更稳定
# 建议: 保持true，除非有特殊的正则化需求

dropout: 0.1                         # Dropout概率
# 说明: 训练时随机将部分神经元输出置零，防止过拟合
# 0.0: 无正则化，可能过拟合，适合大数据集
# 0.1: 轻度正则化，适合大多数情况
# 0.3+: 强正则化，适合小数据集或容易过拟合的场景
# 注意: 推理时自动关闭

# =============================================================================
# 注意力机制配置
# =============================================================================

attention_type: "causal_self_attention" # 注意力类型
# 说明: 因果自注意力确保模型只能看到当前位置之前的token
# 这是GPT类模型的核心特性，支持自回归生成

use_flash_attention: true            # 是否启用Flash Attention优化
# 说明: 内存高效的注意力计算，减少显存使用并加速训练
# true:  显存使用更少，训练更快（需要PyTorch 2.0+）
# false: 标准注意力实现，兼容性更好
# 要求: 需要支持的硬件和PyTorch版本

# =============================================================================
# 嵌入层配置
# =============================================================================

tie_word_embeddings: true            # 输入输出嵌入层权重共享
# 说明: 输入token嵌入和输出语言模型头是否共享权重矩阵
# true:  共享权重，减少约38M参数，通常性能相当
# false: 独立权重，更多参数，理论上表达能力更强
# 建议: 保持true以节省参数和显存

position_embedding_type: "learned"   # 位置嵌入类型
# 说明: 如何编码token在序列中的位置信息
# "learned": 可学习的位置嵌入，GPT-2标准做法
# "sinusoidal": 固定的正弦位置编码，不增加参数
# "rotary": RoPE旋转位置编码，支持长度外推

# =============================================================================
# 长度外推配置 - 支持处理超过训练长度的序列
# =============================================================================

enable_length_extrapolation: true    # 是否启用长度外推功能
# 说明: 允许模型处理比训练时更长的序列
# true:  启用外推，可处理超长文本，但可能有性能下降
# false: 严格限制在训练长度内，性能最佳

extrapolation_method: "linear"       # 长度外推方法
# 说明: 控制如何处理超出训练长度的位置编码
# "linear": 线性插值，简单有效
# "ntk": NTK-aware插值，更好的外推性能
# "yarn": YaRN方法，专为长文本优化
# "rope_scaling": RoPE缩放，需要配合RoPE使用

rope_scaling_factor: 1.0             # RoPE缩放因子
# 说明: 当使用RoPE时的缩放参数，影响外推能力
# 1.0: 无缩放，标准RoPE
# >1.0: 增强外推能力，但可能影响短文本性能
# 建议: 根据目标长度调整，通常1.0-4.0之间

rope_theta: 10000.0                  # RoPE基础频率
# 说明: RoPE旋转位置编码的基础频率参数
# 10000.0: 标准值，适合大多数情况
# 更大值: 更好的长距离建模，但计算开销增加

max_extrapolation_length: 4096       # 最大外推长度
# 说明: 允许外推到的最大序列长度
# 影响: 设置合理上限，防止无限外推导致的性能问题
# 建议: 根据实际需求设置，通常是训练长度的2-4倍

extrapolation_alpha: 1.0             # 外推强度参数
# 说明: 控制外推的激进程度，用于某些外推方法
# 1.0: 标准外推强度
# <1.0: 保守外推，更稳定但外推能力有限
# >1.0: 激进外推，更强外推能力但可能不稳定

# =============================================================================
# 初始化和数值稳定性
# =============================================================================

initializer_range: 0.02              # 权重初始化标准差
# 说明: 模型权重初始化时的正态分布标准差
# 影响: 太大可能导致梯度爆炸，太小可能导致梯度消失
# 0.02是GPT-2的经验值，适合大多数情况

layer_norm_epsilon: 1e-5             # LayerNorm的数值稳定性参数
# 说明: 防止LayerNorm计算中除零错误的小常数
# 影响: 太小可能数值不稳定，太大可能影响归一化效果

# =============================================================================
# 训练和推理优化
# =============================================================================

torch_dtype: "float32"               # 模型参数数据类型
# 说明: 控制模型参数和计算的数值精度
# "float32": 单精度，最高精度，显存需求大
# "float16": 半精度，显存减半，可能有数值不稳定
# "bfloat16": 更稳定的半精度，需要硬件支持

gradient_checkpointing: false        # 梯度检查点优化
# 说明: 用计算时间换显存空间的优化技术
# true:  显存使用减少约50%，但训练时间增加约20%
# false: 训练更快，但需要更多显存
# 建议: 125M模型通常不需要，大模型或显存不足时启用

use_cache: true                      # 推理时是否缓存键值对
# 说明: 生成时缓存注意力的key和value，避免重复计算
# 影响: 显著加速生成，但会占用额外显存

# =============================================================================
# Token配置（与tokenizer对应）
# =============================================================================

pad_token_id: 50256                  # 填充token ID
eos_token_id: 50256                  # 序列结束token ID  
bos_token_id: 50256                  # 序列开始token ID
# 说明: GPT-2使用同一个token(<|endoftext|>)作为特殊token
# 这些ID必须与使用的tokenizer保持一致

# =============================================================================
# 模型规模信息（仅供参考）
# =============================================================================

estimated_parameters: 124439808      # 预估参数量（约125M）
memory_footprint_mb: 500            # 预估显存占用（FP32，仅模型权重）
flops_per_token: 248879616           # 每个token的浮点运算次数

# =============================================================================
# 兼容性字段（某些库可能需要）
# =============================================================================

hidden_size: 768                     # 等同于n_embd
intermediate_size: 3072               # FFN隐藏层大小（4 * n_embd）
max_position_embeddings: 1024        # 等同于block_size
num_attention_heads: 12               # 等同于n_head
num_hidden_layers: 12                 # 等同于n_layer

# =============================================================================
# 输出控制（调试和分析用）
# =============================================================================

use_return_dict: true                # 返回结构化输出
output_attentions: false             # 是否输出注意力权重
output_hidden_states: false          # 是否输出隐藏状态
# 说明: 这些选项主要用于模型分析和调试，训练时通常关闭以节省内存
