# 配置文件系统说明

本目录是整个训练框架的"控制大脑"，采用**代码与配置分离**的设计哲学。所有的实验参数、模型结构、训练设置都通过YAML配置文件进行管理，无需修改任何Python代码。

## 目录结构

```
configs/
├── model/          # 模型结构配置
│   ├── gpt_125m.yaml    # 125M参数模型
│   └── gpt_1b.yaml      # 1B参数模型
├── train/          # 训练任务配置
│   ├── pretrain.yaml    # 预训练配置
│   ├── sft.yaml         # 指令微调配置
│   └── dpo.yaml         # 直接偏好优化配置
└── README.md       # 本文档
```

## 设计哲学

### 配置分离原则
- **模型配置** (`model/`): 定义"模型长什么样" - 层数、头数、维度等物理结构
- **训练配置** (`train/`): 定义"如何训练" - 学习率、批次大小、数据源等

### 数据源配置
训练配置支持两种数据模式：
```yaml
# 本地文件模式 (适合调试)
data:
  type: "local"
  path: "data/alpaca_data.json"

# 流式模式 (适合大规模训练)
data:
  type: "streaming"
  provider: "huggingface"
  path: "tatsu-lab/alpaca"
  subset: "default"
```

---

## 模型配置参数详解

**注意**: `gpt_125m.yaml` 作为完整的配置示例，包含所有参数的详细注释说明。其他配置文件只包含核心参数和简要说明。

### 核心架构参数

#### `n_layer` (层数)
- **作用**: 控制Transformer块的数量，直接影响模型容量和计算复杂度
- **影响**: 
  - 更多层 → 更强表达能力，但训练更慢，容易过拟合
  - 更少层 → 训练快，但表达能力有限
- **典型值**: 12层(125M), 24层(350M), 32层(1.3B)

#### `n_head` (注意力头数)
- **作用**: 多头注意力机制的头数，允许模型关注不同类型的信息
- **影响**:
  - 更多头 → 能捕获更复杂的依赖关系
  - 必须能被`n_embd`整除 (每个头的维度 = n_embd / n_head)
- **典型值**: 12头(125M), 16头(350M), 20头(1.3B)

#### `n_embd` (嵌入维度)
- **作用**: 模型的隐藏状态维度，是模型"宽度"的核心指标
- **影响**: 
  - 更大维度 → 更强表达能力，但显存和计算需求增加
  - 必须能被`n_head`整除
- **典型值**: 768(125M), 1024(350M), 1600(1.3B)

#### `vocab_size` (词汇表大小)
- **作用**: 模型能理解的token数量，决定输入输出层的大小
- **影响**: 
  - 更大词汇表 → 能处理更多语言，但参数量增加
  - 50257是GPT-2标准词汇表大小
- **注意**: 通常不需要修改，除非使用自定义tokenizer

#### `block_size` (序列长度)
- **作用**: 模型能处理的最大输入长度（上下文窗口）
- **影响**:
  - 更长序列 → 能处理更长文本，但显存需求平方级增长
  - 训练时所有序列会被截断或填充到这个长度
- **典型值**: 1024(GPT-2), 2048(现代模型), 4096(长文本模型)

### 训练相关参数

#### `bias` (偏置项)
- **作用**: 控制线性层是否包含偏置项 `y = Wx + b`
- **影响**:
  - `bias: true` → 标准设置，模型表达能力更强
  - `bias: false` → 减少参数量，某些情况下训练更稳定
- **建议**: 除非有特殊需求，保持`true`

#### `dropout` (随机失活)
- **作用**: 训练时随机将部分神经元输出置零，防止过拟合
- **影响**:
  - 0.0 → 无dropout，可能过拟合
  - 0.1 → 轻度正则化，适合大多数情况
  - 0.3+ → 强正则化，适合小数据集
- **注意**: 推理时自动关闭

#### `tie_word_embeddings` (权重共享)
- **作用**: 输入嵌入层和输出层是否共享权重矩阵
- **影响**:
  - `true` → 减少参数量，通常性能相当
  - `false` → 更多参数，理论上表达能力更强
- **建议**: 通常设为`true`以节省参数

### 优化相关参数

#### `use_flash_attention` (Flash Attention)
- **作用**: 是否使用内存优化的注意力计算
- **影响**:
  - `true` → 显存使用更少，训练更快（需要支持）
  - `false` → 标准注意力，兼容性更好
- **要求**: 需要PyTorch 2.0+和兼容硬件

#### `gradient_checkpointing` (梯度检查点)
- **作用**: 用计算时间换显存空间的优化技术
- **影响**:
  - `true` → 显存使用减少，但训练变慢
  - `false` → 训练更快，但需要更多显存
- **建议**: 大模型或显存不足时启用

#### `torch_dtype` (数据类型)
- **作用**: 模型参数的数据精度
- **选项**:
  - `"float32"` → 最高精度，显存需求大
  - `"float16"` → 半精度，显存减半，可能有数值不稳定
  - `"bfloat16"` → 更稳定的半精度（需要硬件支持）

### 长度外推参数

#### `enable_length_extrapolation` (启用长度外推)
- **作用**: 控制模型是否支持处理超过训练长度的序列
- **影响**:
  - `true` → 可处理超长文本，但可能有性能下降
  - `false` → 严格限制在训练长度内，性能最佳
- **应用场景**: 长文档处理、代码生成、学术论文分析

#### `extrapolation_method` (外推方法)
- **作用**: 选择长度外推的具体算法
- **选项**:
  - `"linear"` → 线性插值，简单有效，适合轻度外推
  - `"ntk"` → NTK-aware插值，更好的数学基础
  - `"yarn"` → YaRN方法，专为长文本优化
  - `"rope_scaling"` → RoPE缩放，需要配合RoPE位置编码

#### `rope_scaling_factor` (RoPE缩放因子)
- **作用**: 控制RoPE位置编码的缩放程度
- **影响**:
  - `1.0` → 无缩放，标准行为
  - `2.0` → 支持2倍长度外推
  - `4.0` → 支持4倍长度外推，但短文本性能可能下降
- **建议**: 根据目标长度设置，过大会影响性能

#### `max_extrapolation_length` (最大外推长度)
- **作用**: 设置外推的上限，防止无限外推
- **影响**: 平衡外推能力和计算效率
- **典型值**: 
  - 训练长度1024 → 外推到4096
  - 训练长度2048 → 外推到8192
- **注意**: 外推长度越大，计算开销越大

---

## 创建新配置文件的模板

### 模型配置模板
```yaml
# 模型名称和类型
model_type: "gpt"
model_name: "your_model_name"

# 必需的核心参数
n_layer: 12
n_head: 12  
n_embd: 768
vocab_size: 50257
block_size: 1024

# 可选的优化参数
bias: true
dropout: 0.1
use_flash_attention: true
tie_word_embeddings: true
gradient_checkpointing: false
torch_dtype: "float32"
```

### 训练配置模板
```yaml
# 实验基本信息
experiment_name: "your_experiment"
model_config_path: "configs/model/your_model.yaml"

# 数据配置
data:
  type: "local"  # 或 "streaming"
  path: "path/to/your/data"

# 训练超参数
batch_size: 8
learning_rate: 5e-4
num_epochs: 3
warmup_steps: 500

# 其他设置...
```

---

## 快速开始

1. **选择模型大小**: 根据你的硬件选择`gpt_125m.yaml`或`gpt_1b.yaml`
2. **配置训练任务**: 修改`train/`目录下对应的训练配置
3. **启动训练**: 使用`scripts/`目录下的脚本启动训练

记住：修改配置文件即可改变实验设置，无需修改任何Python代码！
