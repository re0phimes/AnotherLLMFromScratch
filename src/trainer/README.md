# Trainer æ¨¡å—

æœ¬æ¨¡å—æä¾›è®­ç»ƒå™¨å’Œä¼˜åŒ–å™¨çš„å®ç°ï¼ŒåŒ…å«**æ•™å­¦ç‰ˆæœ¬**å’Œ**å®é™…è®­ç»ƒç‰ˆæœ¬**ä¸¤ç§å®ç°æ–¹å¼ã€‚

## ğŸ“‚ ç›®å½•ç»“æ„

```
trainer/
â”œâ”€â”€ tutorial/                          # æ•™å­¦ç‰ˆæœ¬ï¼ˆæ‰‹æ“å®ç°ï¼Œè¯¦ç»†æ³¨é‡Šï¼‰
â”‚   â”œâ”€â”€ __init__.py                   # æ¨¡å—åˆå§‹åŒ–
â”‚   â”œâ”€â”€ optimizer_from_scratch.py     # ä»é›¶å®ç°ä¼˜åŒ–å™¨
â”‚   â””â”€â”€ trainer_from_scratch.py       # ä»é›¶å®ç°è®­ç»ƒå™¨
â”œâ”€â”€ optimizer.py                       # å®é™…è®­ç»ƒç‰ˆæœ¬ï¼ˆç²¾ç®€é«˜æ•ˆï¼‰
â”œâ”€â”€ trainer.py                         # å®é™…è®­ç»ƒç‰ˆæœ¬ï¼ˆç²¾ç®€é«˜æ•ˆï¼‰
â”œâ”€â”€ checkpoint.py                      # æ£€æŸ¥ç‚¹ç®¡ç†å·¥å…·
â””â”€â”€ README.md                          # æœ¬æ–‡æ¡£
```

## ğŸ¯ ä¸¤ç§ç‰ˆæœ¬çš„åŒºåˆ«

### æ•™å­¦ç‰ˆæœ¬ï¼ˆ`tutorial/`ï¼‰

**ç›®çš„**ï¼šå¸®åŠ©ç†è§£æ·±åº¦å­¦ä¹ è®­ç»ƒçš„åº•å±‚åŸç†

**ç‰¹ç‚¹**ï¼š
- âœï¸ ä»é›¶æ‰‹æ“å®ç°ï¼Œä¸ä¾èµ– PyTorch å†…ç½®ä¼˜åŒ–å™¨
- ğŸ“š åŒ…å«è¯¦ç»†çš„æ•°å­¦å…¬å¼å’Œå®ç°åŸç†æ³¨é‡Š
- ğŸ” é€æ­¥è§£é‡Šæ¯ä¸ªæ­¥éª¤çš„ä½œç”¨
- ğŸ“ é€‚åˆå­¦ä¹ å’Œæ•™å­¦

**åŒ…å«å†…å®¹**ï¼š
- `AdamWFromScratch`: æ‰‹æ“ AdamW ä¼˜åŒ–å™¨ï¼ˆå¸¦å…¬å¼æ¨å¯¼ï¼‰
- `SGDFromScratch`: æ‰‹æ“ SGD ä¼˜åŒ–å™¨ï¼ˆå¸¦åŠ¨é‡ï¼‰
- `TrainerFromScratch`: æ‰‹æ“è®­ç»ƒå¾ªç¯ï¼ˆè¯¦ç»†æ­¥éª¤æ³¨é‡Šï¼‰

**ç¤ºä¾‹**ï¼š
```python
from src.trainer.tutorial import AdamWFromScratch, TrainerFromScratch

# ä½¿ç”¨æ‰‹æ“çš„ä¼˜åŒ–å™¨
optimizer = AdamWFromScratch(
    params=model.parameters(),
    lr=1e-3,
    betas=(0.9, 0.999),
    weight_decay=0.01
)

# ä½¿ç”¨æ‰‹æ“çš„è®­ç»ƒå™¨
trainer = TrainerFromScratch(
    model=model,
    optimizer=optimizer,
    train_loader=train_loader,
    val_loader=val_loader
)

trainer.train()
```

### å®é™…è®­ç»ƒç‰ˆæœ¬ï¼ˆæ ¹ç›®å½•ï¼‰

**ç›®çš„**ï¼šæä¾›é«˜æ•ˆã€å¯é çš„è®­ç»ƒå®ç°

**ç‰¹ç‚¹**ï¼š
- âš¡ ä½¿ç”¨ PyTorch å†…ç½®ä¼˜åŒ–å™¨å’Œå·¥å…·
- ğŸš€ ä»£ç ç²¾ç®€ï¼Œæ€§èƒ½ä¼˜åŒ–
- ğŸ¯ é€‚åˆå®é™…é¡¹ç›®ä½¿ç”¨
- ğŸ› ï¸ æä¾›ä¾¿æ·çš„å·¥å‚å‡½æ•°å’Œé…ç½®é€‰é¡¹

**åŒ…å«å†…å®¹**ï¼š
- `create_optimizer()`: ä¼˜åŒ–å™¨å·¥å‚å‡½æ•°
- `configure_optimizer()`: å¸¦å‚æ•°åˆ†ç»„çš„ä¼˜åŒ–å™¨é…ç½®
- `Trainer`: é«˜æ•ˆçš„è®­ç»ƒå™¨ç±»

**ç¤ºä¾‹**ï¼š
```python
from src.trainer.optimizer import configure_optimizer
from src.trainer.trainer import Trainer

# ä½¿ç”¨å·¥å‚å‡½æ•°åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆè‡ªåŠ¨å‚æ•°åˆ†ç»„ï¼‰
optimizer = configure_optimizer(
    model=model,
    optimizer_type='adamw',
    lr=1e-3,
    weight_decay=0.01
)

# ä½¿ç”¨ç²¾ç®€çš„è®­ç»ƒå™¨
from torch.optim.lr_scheduler import CosineAnnealingLR

scheduler = CosineAnnealingLR(optimizer, T_max=1000)

trainer = Trainer(
    model=model,
    optimizer=optimizer,
    train_loader=train_loader,
    val_loader=val_loader,
    scheduler=scheduler,
    device='cuda',
    max_epochs=10,
    grad_accum_steps=4,
    use_amp=True
)

trainer.train()
```

## ğŸ“– æ¨èå­¦ä¹ è·¯å¾„

### ç¬¬ä¸€æ­¥ï¼šç†è§£ä¼˜åŒ–å™¨åŸç†
é˜…è¯» `tutorial/optimizer_from_scratch.py`

**å­¦ä¹ è¦ç‚¹**ï¼š
1. Adam/AdamW çš„æ•°å­¦åŸç†
2. ä¸€é˜¶çŸ©å’ŒäºŒé˜¶çŸ©çš„è®¡ç®—
3. åå·®ä¿®æ­£çš„ä½œç”¨
4. æƒé‡è¡°å‡çš„å®ç°æ–¹å¼

### ç¬¬äºŒæ­¥ï¼šç†è§£è®­ç»ƒå¾ªç¯
é˜…è¯» `tutorial/trainer_from_scratch.py`

**å­¦ä¹ è¦ç‚¹**ï¼š
1. å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­
2. æ¢¯åº¦ç´¯ç§¯çš„åŸç†
3. æ¢¯åº¦è£å‰ªé˜²æ­¢çˆ†ç‚¸
4. æ··åˆç²¾åº¦è®­ç»ƒ
5. å­¦ä¹ ç‡è°ƒåº¦

### ç¬¬ä¸‰æ­¥ï¼šå­¦ä¹ å·¥ç¨‹å®è·µ
é˜…è¯» `optimizer.py` å’Œ `trainer.py`

**å­¦ä¹ è¦ç‚¹**ï¼š
1. å¦‚ä½•ä½¿ç”¨ PyTorch å†…ç½®å·¥å…·
2. å‚æ•°åˆ†ç»„çš„æœ€ä½³å®è·µ
3. é«˜æ•ˆçš„è®­ç»ƒå¾ªç¯å®ç°
4. æ£€æŸ¥ç‚¹ç®¡ç†

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ä½¿ç”¨æ•™å­¦ç‰ˆæœ¬ï¼ˆå­¦ä¹ ç”¨ï¼‰

```python
from src.trainer.tutorial import AdamWFromScratch, TrainerFromScratch

# åˆ›å»ºä¼˜åŒ–å™¨
optimizer = AdamWFromScratch(
    params=model.parameters(),
    lr=1e-3,
    weight_decay=0.01
)

# åˆ›å»ºè®­ç»ƒå™¨
trainer = TrainerFromScratch(
    model=model,
    optimizer=optimizer,
    train_loader=train_loader,
    val_loader=val_loader,
    device='cuda',
    max_epochs=10
)

# å¼€å§‹è®­ç»ƒ
trainer.train()
```

### ä½¿ç”¨å®é™…ç‰ˆæœ¬ï¼ˆå®é™…é¡¹ç›®ï¼‰

```python
from src.trainer.optimizer import configure_optimizer
from src.trainer.trainer import Trainer
from torch.optim.lr_scheduler import CosineAnnealingLR

# é…ç½®ä¼˜åŒ–å™¨ï¼ˆè‡ªåŠ¨å‚æ•°åˆ†ç»„ï¼‰
optimizer = configure_optimizer(
    model=model,
    optimizer_type='adamw',
    lr=1e-3,
    weight_decay=0.01
)

# åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
scheduler = CosineAnnealingLR(optimizer, T_max=1000)

# åˆ›å»ºè®­ç»ƒå™¨
trainer = Trainer(
    model=model,
    optimizer=optimizer,
    train_loader=train_loader,
    val_loader=val_loader,
    scheduler=scheduler,
    device='cuda',
    max_epochs=10,
    grad_accum_steps=4,
    max_grad_norm=1.0,
    use_amp=True,
    log_interval=100,
    save_dir='./checkpoints'
)

# å¼€å§‹è®­ç»ƒ
trainer.train()
```

## ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µè§£é‡Š

### 1. æ¢¯åº¦ç´¯ç§¯ï¼ˆGradient Accumulationï¼‰

**ä½œç”¨**ï¼šæ¨¡æ‹Ÿæ›´å¤§çš„ batch sizeï¼Œåœ¨æ˜¾å­˜æœ‰é™æ—¶å¾ˆæœ‰ç”¨

**åŸç†**ï¼š
```python
# ç´¯ç§¯ N æ­¥çš„æ¢¯åº¦
loss = loss / N
loss.backward()  # ç´¯ç§¯æ¢¯åº¦

# æ¯ N æ­¥æ›´æ–°ä¸€æ¬¡å‚æ•°
if step % N == 0:
    optimizer.step()
    optimizer.zero_grad()
```

### 2. æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰

**ä½œç”¨**ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œç¨³å®šè®­ç»ƒ

**åŸç†**ï¼š
```python
# å¦‚æœæ¢¯åº¦èŒƒæ•° > max_normï¼Œç¼©æ”¾æ¢¯åº¦
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

### 3. æ··åˆç²¾åº¦è®­ç»ƒï¼ˆMixed Precisionï¼‰

**ä½œç”¨**ï¼šåŠ é€Ÿè®­ç»ƒï¼ŒèŠ‚çœæ˜¾å­˜

**åŸç†**ï¼š
```python
with autocast('cuda'):
    outputs = model(inputs)
    loss = criterion(outputs, labels)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### 4. å‚æ•°åˆ†ç»„ï¼ˆParameter Groupsï¼‰

**ä½œç”¨**ï¼šå¯¹ä¸åŒå‚æ•°åº”ç”¨ä¸åŒçš„ä¼˜åŒ–ç­–ç•¥

**æœ€ä½³å®è·µ**ï¼š
- bias å’Œ LayerNorm å‚æ•°é€šå¸¸ä¸åº”ç”¨æƒé‡è¡°å‡
- æƒé‡çŸ©é˜µåº”ç”¨æƒé‡è¡°å‡

```python
param_groups = [
    {'params': decay_params, 'weight_decay': 0.01},
    {'params': no_decay_params, 'weight_decay': 0.0}
]
optimizer = AdamW(param_groups, lr=1e-3)
```

### 5. å­¦ä¹ ç‡è°ƒåº¦ï¼ˆLearning Rate Schedulingï¼‰

**å¸¸ç”¨ç­–ç•¥**ï¼š
- **Warmup**ï¼šè®­ç»ƒåˆæœŸçº¿æ€§å¢é•¿å­¦ä¹ ç‡
- **Cosine Annealing**ï¼šä½™å¼¦å‡½æ•°è¡°å‡
- **Step Decay**ï¼šæ¯éš”ä¸€å®šæ­¥æ•°é™ä½å­¦ä¹ ç‡

```python
# PyTorch æä¾›çš„è°ƒåº¦å™¨
from torch.optim.lr_scheduler import (
    CosineAnnealingLR,      # ä½™å¼¦é€€ç«
    CosineAnnealingWarmRestarts,  # å¸¦é‡å¯çš„ä½™å¼¦é€€ç«
    OneCycleLR,             # One Cycle ç­–ç•¥
    ReduceLROnPlateau       # åŸºäºéªŒè¯é›†çš„è‡ªé€‚åº”è°ƒæ•´
)
```

## ğŸ“Š æ£€æŸ¥ç‚¹ç®¡ç†

è®­ç»ƒå™¨è‡ªåŠ¨ä¿å­˜æ£€æŸ¥ç‚¹ï¼ŒåŒ…å«ï¼š
- æ¨¡å‹æƒé‡
- ä¼˜åŒ–å™¨çŠ¶æ€
- è°ƒåº¦å™¨çŠ¶æ€
- è®­ç»ƒè¿›åº¦ï¼ˆepoch, stepï¼‰
- æœ€ä½³éªŒè¯æŸå¤±

```python
# ä¿å­˜æ£€æŸ¥ç‚¹
trainer.save_checkpoint(is_best=True)

# åŠ è½½æ£€æŸ¥ç‚¹
trainer.load_checkpoint('./checkpoints/best_model.pt')

# ç»§ç»­è®­ç»ƒ
trainer.train()
```

## ğŸ“ æ•™å­¦èµ„æº

### æ¨èé˜…è¯»é¡ºåº

1. **ä¼˜åŒ–å™¨åŸºç¡€**
   - `tutorial/optimizer_from_scratch.py` ä¸­çš„ SGD å®ç°
   - ç†è§£æ¢¯åº¦ä¸‹é™å’ŒåŠ¨é‡

2. **AdamW ä¼˜åŒ–å™¨**
   - `tutorial/optimizer_from_scratch.py` ä¸­çš„ AdamW å®ç°
   - ç†è§£è‡ªé€‚åº”å­¦ä¹ ç‡å’Œæƒé‡è¡°å‡

3. **è®­ç»ƒå¾ªç¯**
   - `tutorial/trainer_from_scratch.py` çš„å®Œæ•´å®ç°
   - ç†è§£è®­ç»ƒçš„å®Œæ•´æµç¨‹

4. **å·¥ç¨‹å®è·µ**
   - `optimizer.py` çš„å·¥å‚æ¨¡å¼å’Œå‚æ•°åˆ†ç»„
   - `trainer.py` çš„é«˜æ•ˆå®ç°

### å®éªŒå»ºè®®

1. **å¯¹æ¯”å®éªŒ**ï¼šä½¿ç”¨ç›¸åŒè¶…å‚æ•°ï¼Œå¯¹æ¯”æ‰‹æ“ç‰ˆæœ¬å’Œ PyTorch ç‰ˆæœ¬çš„ç»“æœ
2. **å‚æ•°è°ƒä¼˜**ï¼šå°è¯•ä¸åŒçš„å­¦ä¹ ç‡ã€æƒé‡è¡°å‡ã€æ¢¯åº¦è£å‰ªé˜ˆå€¼
3. **æ€§èƒ½åˆ†æ**ï¼šæµ‹é‡æ··åˆç²¾åº¦è®­ç»ƒçš„åŠ é€Ÿæ¯”å’Œæ˜¾å­˜èŠ‚çœ
4. **å¯è§†åŒ–**ï¼šä½¿ç”¨ tensorboard æˆ– wandb è®°å½•è®­ç»ƒæ›²çº¿

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **æ•™å­¦ç‰ˆæœ¬ä»…ç”¨äºå­¦ä¹ **ï¼Œå®é™…é¡¹ç›®è¯·ä½¿ç”¨ç²¾ç®€ç‰ˆæœ¬
2. **å‚æ•°åˆ†ç»„**æ˜¯è®­ç»ƒ Transformer çš„æœ€ä½³å®è·µ
3. **æ··åˆç²¾åº¦**åœ¨ Ampere æ¶æ„ï¼ˆRTX 30 ç³»åˆ—ï¼‰åŠä»¥ä¸Šæœ‰æ˜¾è‘—åŠ é€Ÿ
4. **æ¢¯åº¦è£å‰ª**å¯¹äº RNN/LSTM/Transformer è®­ç»ƒå¾ˆé‡è¦
5. **å­¦ä¹ ç‡é¢„çƒ­**å¯ä»¥æé«˜è®­ç»ƒç¨³å®šæ€§

## ğŸ”— ç›¸å…³èµ„æº

- [Adam è®ºæ–‡](https://arxiv.org/abs/1412.6980)
- [AdamW è®ºæ–‡](https://arxiv.org/abs/1711.05101)
- [æ··åˆç²¾åº¦è®­ç»ƒ](https://pytorch.org/docs/stable/amp.html)
- [æ¢¯åº¦è£å‰ª](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)

---

**ä½œè€…**ï¼šAnotherLLMFromScratch é¡¹ç›®  
**æ›´æ–°æ—¶é—´**ï¼š2025-10-11
