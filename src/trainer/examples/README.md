# Trainer ç¤ºä¾‹ä»£ç 

æœ¬ç›®å½•åŒ…å«è®­ç»ƒå™¨çš„ç¤ºä¾‹ä»£ç å’Œå‚è€ƒå®ç°ã€‚

## ğŸ“ æ–‡ä»¶è¯´æ˜

### `basic_trainer_example.py`
è¿™æ˜¯æœ€åˆçš„è®­ç»ƒå™¨å®ç°ï¼Œå±•ç¤ºäº†ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒå™¨åº”è¯¥åŒ…å«çš„æ‰€æœ‰ç»„ä»¶ï¼š
- åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
- æ··åˆç²¾åº¦è®­ç»ƒ
- æ¢¯åº¦ç´¯ç§¯å’Œè£å‰ª
- æ£€æŸ¥ç‚¹ç®¡ç†
- å®Œæ•´çš„è®­ç»ƒå¾ªç¯

**ç”¨é€”**ï¼šä½œä¸ºå­¦ä¹ å‚è€ƒï¼Œäº†è§£è®­ç»ƒå™¨çš„å®Œæ•´å®ç°ç»†èŠ‚ã€‚

**æ³¨æ„**ï¼šå®é™…é¡¹ç›®ä¸­åº”ä½¿ç”¨æ–°çš„æ¨¡å—åŒ–è®­ç»ƒå™¨ï¼ˆ`SFTTrainer`ã€`DPOTrainer` ç­‰ï¼‰ã€‚

---

### `distributed_train_example.py`
å®Œæ•´çš„åˆ†å¸ƒå¼è®­ç»ƒç¤ºä¾‹ï¼Œå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨è®­ç»ƒå™¨è¿›è¡Œå•æœºå¤šå¡æˆ–å¤šæœºå¤šå¡è®­ç»ƒã€‚

**åŒ…å«å†…å®¹**ï¼š
- åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–
- `DistributedSampler` çš„ä½¿ç”¨
- æ•°æ®åŠ è½½å™¨é…ç½®
- æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨çš„åˆ›å»º
- å®Œæ•´çš„è®­ç»ƒæµç¨‹

**è¿è¡Œæ–¹å¼**ï¼š

```bash
# å•æœºå•å¡ï¼ˆæ™®é€šè®­ç»ƒï¼‰
python distributed_train_example.py

# å•æœº4å¡
torchrun --nproc_per_node=4 distributed_train_example.py

# å¤šæœºå¤šå¡ï¼ˆ2æœº8å¡ï¼Œæ¯æœº4å¡ï¼‰
# ä¸»èŠ‚ç‚¹ï¼ˆèŠ‚ç‚¹0ï¼‰
torchrun --nproc_per_node=4 --nnodes=2 --node_rank=0 \
         --master_addr="192.168.1.1" --master_port=29500 \
         distributed_train_example.py

# ä»èŠ‚ç‚¹ï¼ˆèŠ‚ç‚¹1ï¼‰
torchrun --nproc_per_node=4 --nnodes=2 --node_rank=1 \
         --master_addr="192.168.1.1" --master_port=29500 \
         distributed_train_example.py
```

---

## ğŸš€ å¦‚ä½•ä½¿ç”¨æ–°çš„è®­ç»ƒå™¨

### 1. SFT è®­ç»ƒï¼ˆç›‘ç£å¾®è°ƒï¼‰

```python
from src.trainer import SFTTrainer
from src.trainer.optimizer import configure_optimizer

# é…ç½®ä¼˜åŒ–å™¨
optimizer = configure_optimizer(
    model=model,
    optimizer_type='adamw',
    lr=1e-4,
    weight_decay=0.01
)

# åˆ›å»ºè®­ç»ƒå™¨
trainer = SFTTrainer(
    model=model,
    optimizer=optimizer,
    train_loader=train_loader,
    val_loader=val_loader,
    device='cuda',
    max_epochs=10,
    grad_accum_steps=4,
    use_amp=True
)

# å¼€å§‹è®­ç»ƒ
trainer.train()
```

### 2. DPO è®­ç»ƒï¼ˆåå¥½å¯¹é½ï¼‰

```python
import copy
from src.trainer import DPOTrainer

# åˆ›å»ºå‚è€ƒæ¨¡å‹ï¼ˆSFT åçš„æ¨¡å‹å‰¯æœ¬ï¼‰
ref_model = copy.deepcopy(model)
ref_model.eval()

# åˆ›å»º DPO è®­ç»ƒå™¨
trainer = DPOTrainer(
    model=model,
    ref_model=ref_model,
    optimizer=optimizer,
    train_loader=dpo_train_loader,  # DPO æ ¼å¼æ•°æ®
    val_loader=dpo_val_loader,
    beta=0.1,  # DPO æ¸©åº¦å‚æ•°
    device='cuda',
    max_epochs=3
)

# å¼€å§‹è®­ç»ƒ
trainer.train()
```

### 3. GRPO è®­ç»ƒï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰

```python
from src.trainer import GRPOTrainer

# æ³¨æ„ï¼šGRPO éœ€è¦è‡ªå®šä¹‰é‡‡æ ·å’Œå¥–åŠ±é€»è¾‘
# è¿™é‡Œå±•ç¤ºåŸºæœ¬ç”¨æ³•ï¼Œå®é™…ä½¿ç”¨éœ€è¦æä¾› responses å’Œ rewards

trainer = GRPOTrainer(
    model=model,
    optimizer=optimizer,
    train_loader=train_loader,  # éœ€è¦åŒ…å« responses å’Œ rewards
    num_samples_per_prompt=4,
    clip_eps=0.2,
    device='cuda',
    max_epochs=5
)

# å¼€å§‹è®­ç»ƒ
trainer.train()
```

### 4. åˆ†å¸ƒå¼è®­ç»ƒï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰

æ‰€æœ‰è®­ç»ƒå™¨éƒ½æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒï¼Œæ— éœ€é¢å¤–é…ç½®ï¼š

```bash
# ä½¿ç”¨ torchrun å¯åŠ¨
torchrun --nproc_per_node=4 train_sft.py

# è®­ç»ƒå™¨ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶å¯ç”¨ DDP
```

---

## ğŸ“š æ›´å¤šèµ„æº

- **æ•™å­¦ç‰ˆæœ¬**ï¼šæŸ¥çœ‹ `../tutorial/` ç›®å½•äº†è§£ä»é›¶å®ç°çš„è®­ç»ƒå™¨
- **å®é™…è®­ç»ƒ**ï¼šæŸ¥çœ‹çˆ¶ç›®å½•çš„æ¨¡å—åŒ–è®­ç»ƒå™¨å®ç°
- **æ–‡æ¡£**ï¼šæŸ¥çœ‹ `../README.md` äº†è§£å®Œæ•´çš„æ¨¡å—æ–‡æ¡£

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **ç¤ºä¾‹ä»£ç ä»…ä¾›å‚è€ƒ**ï¼šå®é™…é¡¹ç›®è¯·ä½¿ç”¨æ¨¡å—åŒ–çš„è®­ç»ƒå™¨ï¼ˆ`SFTTrainer` ç­‰ï¼‰
2. **æ•°æ®æ ¼å¼**ï¼šç¡®ä¿æ•°æ®åŠ è½½å™¨è¿”å›çš„ batch æ ¼å¼ä¸è®­ç»ƒå™¨è¦æ±‚ä¸€è‡´
3. **åˆ†å¸ƒå¼è®­ç»ƒ**ï¼šä½¿ç”¨ `DistributedSampler` ç¡®ä¿æ•°æ®ä¸é‡å¤
4. **æ£€æŸ¥ç‚¹**ï¼šåªæœ‰ä¸»è¿›ç¨‹ä¿å­˜æ£€æŸ¥ç‚¹ï¼Œæ‰€æœ‰è¿›ç¨‹åŠ è½½æ£€æŸ¥ç‚¹

---

**æ›´æ–°æ—¶é—´**ï¼š2025-10-11  
**ä½œè€…**ï¼šAnotherLLMFromScratch é¡¹ç›®

