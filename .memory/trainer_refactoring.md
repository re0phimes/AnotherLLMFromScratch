# Trainer æ¨¡å—é‡æ„å®Œæˆè®°å½•

**æ—¥æœŸ**: 2025-10-11  
**ä»»åŠ¡**: é‡æ„ Trainer æ¨¡å—ï¼Œæ”¯æŒå¤šç§è®­ç»ƒèŒƒå¼ï¼ˆSFTã€DPOã€GRPOï¼‰

---

## âœ… å®Œæˆçš„å·¥ä½œ

### 1. åˆ›å»ºæ¨¡å—åŒ–è®­ç»ƒå™¨æ¶æ„

#### 1.1 åŸºç¡€è®­ç»ƒå™¨ (`base_trainer.py`)
- âœ… æŠ½è±¡åŸºç±»ï¼ŒåŒ…å«æ‰€æœ‰é€šç”¨è®­ç»ƒé€»è¾‘
- âœ… åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒï¼ˆè‡ªåŠ¨æ£€æµ‹ DDPï¼‰
- âœ… æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰
- âœ… æ¢¯åº¦ç´¯ç§¯å’Œæ¢¯åº¦è£å‰ª
- âœ… å­¦ä¹ ç‡è°ƒåº¦
- âœ… æ£€æŸ¥ç‚¹ç®¡ç†ï¼ˆä¿å­˜/åŠ è½½ï¼‰
- âœ… å®šä¹‰æŠ½è±¡æ–¹æ³•ä¾›å­ç±»å®ç°ï¼š
  - `_prepare_batch()`: å‡†å¤‡æ‰¹æ¬¡æ•°æ®
  - `_forward()`: å‰å‘ä¼ æ’­
  - `_compute_loss()`: è®¡ç®—æŸå¤±

**æ ¸å¿ƒè®¾è®¡**ï¼š
- æ‰€æœ‰é€šç”¨é€»è¾‘åœ¨åŸºç±»ä¸­å®ç°
- ä»»åŠ¡ç‰¹å®šé€»è¾‘ç”±å­ç±»å®ç°
- æœ€å¤§åŒ–ä»£ç å¤ç”¨

#### 1.2 SFT è®­ç»ƒå™¨ (`sft_trainer.py`)
- âœ… ç»§æ‰¿ `BaseTrainer`
- âœ… å®ç°æ ‡å‡†äº¤å‰ç†µæŸå¤±
- âœ… æ”¯æŒæŒ‡ä»¤å¾®è°ƒå’Œé¢„è®­ç»ƒ
- âœ… è‡ªåŠ¨å¤„ç† prompt éƒ¨åˆ†çš„ label (-100)
- âœ… æä¾› `PretrainTrainer` åˆ«å

**æ•°æ®æ ¼å¼**ï¼š
```python
{
    'input_ids': [B, L],
    'labels': [B, L],  # prompt éƒ¨åˆ†ä¸º -100
    'attention_mask': [B, L]  # å¯é€‰
}
```

#### 1.3 DPO è®­ç»ƒå™¨ (`dpo_trainer.py`)
- âœ… ç»§æ‰¿ `BaseTrainer`
- âœ… å®ç° DPO å¯¹æ¯”æŸå¤±
- âœ… ç»´æŠ¤å†»ç»“çš„å‚è€ƒæ¨¡å‹ï¼ˆreference modelï¼‰
- âœ… å¯¹ chosen å’Œ rejected åˆ†åˆ«å‰å‘ä¼ æ’­
- âœ… æ”¯æŒæ ‡ç­¾å¹³æ»‘
- âœ… å¯é…ç½® beta æ¸©åº¦å‚æ•°

**æ•°æ®æ ¼å¼**ï¼š
```python
{
    'prompt_input_ids': [B, L],
    'chosen_input_ids': [B, L],
    'rejected_input_ids': [B, L],
    'prompt_attention_mask': [B, L],
    'chosen_attention_mask': [B, L],
    'rejected_attention_mask': [B, L]
}
```

**æ ¸å¿ƒé€»è¾‘**ï¼š
- Policy model å’Œ reference model éƒ½å‰å‘ä¼ æ’­
- è®¡ç®— log æ¦‚ç‡å·®å€¼
- DPO æŸå¤±ï¼š`-log(Ïƒ(Î² * (Ï€_Î¸ - Ï€_ref)))`

#### 1.4 GRPO è®­ç»ƒå™¨ (`grpo_trainer.py`)
- âœ… ç»§æ‰¿ `BaseTrainer`
- âœ… å®ç° PPO é£æ ¼çš„ç­–ç•¥ä¼˜åŒ–
- âœ… æ”¯æŒåœ¨çº¿é‡‡æ ·ï¼ˆç”Ÿæˆå›ç­”ï¼‰
- âœ… ç»„å†…å¯¹æ¯”è®¡ç®—ä¼˜åŠ¿å‡½æ•°
- âœ… æ”¯æŒå¥–åŠ±æ¨¡å‹æˆ–è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°
- âœ… åŒ…å« KL æƒ©ç½šå’Œç†µå¥–åŠ±

**æ•°æ®æ ¼å¼**ï¼š
```python
{
    'prompts': [B, L],  # è¾“å…¥
    # ä»¥ä¸‹å¯ä»¥åœ¨çº¿ç”Ÿæˆï¼š
    'responses': [B, L],
    'rewards': [B],
    'old_logprobs': [B]
}
```

**æ ¸å¿ƒé€»è¾‘**ï¼š
- æ¯ä¸ª prompt é‡‡æ ·å¤šä¸ªå›ç­”
- è®¡ç®—å¥–åŠ±å’Œä¼˜åŠ¿å‡½æ•°
- PPO è£å‰ªæŸå¤± + KL æƒ©ç½š + ç†µå¥–åŠ±

---

### 2. æ¨¡å—ç»„ç»‡ç»“æ„

```
src/trainer/
â”œâ”€â”€ base_trainer.py          # åŸºç±»ï¼ˆ600+ è¡Œï¼‰
â”œâ”€â”€ sft_trainer.py           # SFT è®­ç»ƒå™¨ï¼ˆ300+ è¡Œï¼‰
â”œâ”€â”€ dpo_trainer.py           # DPO è®­ç»ƒå™¨ï¼ˆ500+ è¡Œï¼‰
â”œâ”€â”€ grpo_trainer.py          # GRPO è®­ç»ƒå™¨ï¼ˆ600+ è¡Œï¼‰
â”œâ”€â”€ __init__.py              # æ¨¡å—åˆå§‹åŒ–å’Œå¯¼å‡º
â”œâ”€â”€ optimizer.py             # ä¼˜åŒ–å™¨å·¥å…·ï¼ˆå·²æœ‰ï¼‰
â”œâ”€â”€ checkpoint.py            # æ£€æŸ¥ç‚¹å·¥å…·ï¼ˆå·²æœ‰ï¼‰
â”œâ”€â”€ tutorial/                # æ•™å­¦ç‰ˆæœ¬ï¼ˆå·²æœ‰ï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ optimizer_from_scratch.py
â”‚   â””â”€â”€ trainer_from_scratch.py
â””â”€â”€ examples/                # ç¤ºä¾‹ä»£ç ï¼ˆæ–°å¢ï¼‰
    â”œâ”€â”€ README.md
    â”œâ”€â”€ basic_trainer_example.py      # åŸ trainer.py
    â””â”€â”€ distributed_train_example.py   # åˆ†å¸ƒå¼ç¤ºä¾‹
```

---

### 3. æ¨¡å—æ¥å£è®¾è®¡

#### 3.1 ç»Ÿä¸€çš„å¯¼å…¥æ¥å£

```python
from src.trainer import (
    BaseTrainer,
    SFTTrainer,
    PretrainTrainer,
    DPOTrainer,
    GRPOTrainer,
    configure_optimizer,
    get_trainer_class,
    create_trainer
)
```

#### 3.2 å·¥å‚å‡½æ•°

```python
# æ–¹å¼ 1ï¼šç›´æ¥åˆ›å»º
trainer = SFTTrainer(model, optimizer, train_loader)

# æ–¹å¼ 2ï¼šä½¿ç”¨å·¥å‚å‡½æ•°
trainer = create_trainer(
    'sft',
    model=model,
    optimizer=optimizer,
    train_loader=train_loader
)

# æ–¹å¼ 3ï¼šåŠ¨æ€é€‰æ‹©
TrainerClass = get_trainer_class('dpo')
trainer = TrainerClass(...)
```

---

## ğŸ“Š è®­ç»ƒå™¨å¯¹æ¯”

| ç‰¹æ€§ | SFT | DPO | GRPO |
|-----|-----|-----|------|
| æŸå¤±å‡½æ•° | äº¤å‰ç†µ | å¯¹æ¯”æŸå¤± | PPO + KL |
| æ•°æ®ç±»å‹ | æŒ‡ä»¤-å›ç­” | åå¥½å¯¹æ¯” | æç¤ºï¼ˆåœ¨çº¿ç”Ÿæˆï¼‰|
| å‚è€ƒæ¨¡å‹ | âŒ | âœ… éœ€è¦ | âŒ |
| å¥–åŠ±æ¨¡å‹ | âŒ | âŒ | âœ… å¯é€‰ |
| åœ¨çº¿é‡‡æ · | âŒ | âŒ | âœ… éœ€è¦ |
| è®­ç»ƒé€Ÿåº¦ | å¿« | ä¸­ | æ…¢ |
| æ˜¾å­˜å ç”¨ | ä½ | ä¸­ï¼ˆ2xæ¨¡å‹ï¼‰| ä¸­ |
| é€‚ç”¨åœºæ™¯ | æŒ‡ä»¤å¾®è°ƒ | åå¥½å¯¹é½ | RLHF |

---

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### SFT (Supervised Fine-Tuning)
**é€‚ç”¨äº**ï¼š
- æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰
- é¢„è®­ç»ƒï¼ˆPretrainingï¼‰
- ä»»åŠ¡ç‰¹å®šå¾®è°ƒ

**æ•°æ®éœ€æ±‚**ï¼š
- æŒ‡ä»¤-å›ç­”å¯¹
- é«˜è´¨é‡çš„æ ‡æ³¨æ•°æ®

### DPO (Direct Preference Optimization)
**é€‚ç”¨äº**ï¼š
- åå¥½å¯¹é½ï¼ˆPreference Alignmentï¼‰
- RLHF çš„ç®€åŒ–æ›¿ä»£
- æ¨¡å‹è¡Œä¸ºè°ƒæ•´

**æ•°æ®éœ€æ±‚**ï¼š
- åå¥½å¯¹æ¯”æ•°æ®ï¼ˆchosen vs rejectedï¼‰
- å¯ä»¥ä»å¤šä¸ªæ¨¡å‹è¾“å‡ºä¸­æ„å»º

### GRPO (Group Relative Policy Optimization)
**é€‚ç”¨äº**ï¼š
- å¼ºåŒ–å­¦ä¹ å¼å¯¹é½
- éœ€è¦æ¢ç´¢çš„åœºæ™¯
- å¤æ‚çš„å¥–åŠ±å‡½æ•°

**æ•°æ®éœ€æ±‚**ï¼š
- æç¤ºï¼ˆpromptsï¼‰
- å¥–åŠ±æ¨¡å‹æˆ–å¥–åŠ±å‡½æ•°

---

## ğŸ”„ è®­ç»ƒæµç¨‹ç¤ºä¾‹

### å…¸å‹çš„ä¸‰é˜¶æ®µè®­ç»ƒ

```
é˜¶æ®µ 1: é¢„è®­ç»ƒ
â””â”€> PretrainTrainer (å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®)

é˜¶æ®µ 2: æŒ‡ä»¤å¾®è°ƒ
â””â”€> SFTTrainer (æŒ‡ä»¤-å›ç­”æ•°æ®)

é˜¶æ®µ 3: åå¥½å¯¹é½ï¼ˆé€‰æ‹©å…¶ä¸€ï¼‰
â”œâ”€> DPOTrainer (ç¦»çº¿åå¥½æ•°æ®)
â””â”€> GRPOTrainer (åœ¨çº¿å¼ºåŒ–å­¦ä¹ )
```

---

## ğŸ“ å…³é”®å®ç°ç»†èŠ‚

### 1. æŠ½è±¡æ–¹æ³•è®¾è®¡

åŸºç±»å®šä¹‰ä¸‰ä¸ªæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°ï¼š

```python
class BaseTrainer(ABC):
    @abstractmethod
    def _prepare_batch(self, batch):
        """å‡†å¤‡æ•°æ®ï¼Œç§»åˆ°è®¾å¤‡"""
        pass
    
    @abstractmethod
    def _forward(self, batch):
        """å‰å‘ä¼ æ’­ï¼Œè¿”å›è¾“å‡º"""
        pass
    
    @abstractmethod
    def _compute_loss(self, outputs, batch):
        """è®¡ç®—æŸå¤±"""
        pass
```

### 2. æ‰©å±•ç‚¹è®¾è®¡

åŸºç±»æä¾›å¯é€‰çš„æ‰©å±•ç‚¹ï¼š

```python
class BaseTrainer:
    def _get_extra_config_info(self):
        """å­ç±»æ·»åŠ é…ç½®ä¿¡æ¯"""
        return {}
    
    def _get_extra_log_info(self, batch):
        """å­ç±»æ·»åŠ æ—¥å¿—ä¿¡æ¯"""
        return ""
    
    def _get_extra_checkpoint_state(self):
        """å­ç±»ä¿å­˜é¢å¤–çŠ¶æ€"""
        return None
    
    def _count_tokens(self, batch):
        """å­ç±»è‡ªå®šä¹‰ token è®¡æ•°"""
        return batch['input_ids'].numel()
```

### 3. åˆ†å¸ƒå¼è®­ç»ƒå¤„ç†

æ‰€æœ‰è®­ç»ƒå™¨è‡ªåŠ¨æ”¯æŒåˆ†å¸ƒå¼ï¼š

```python
# è‡ªåŠ¨æ£€æµ‹ç¯å¢ƒ
self.is_distributed = is_distributed()
self.rank = get_rank()

# è‡ªåŠ¨åŒ…è£…æ¨¡å‹
if self.use_ddp:
    self.model = nn.parallel.DistributedDataParallel(...)

# è‡ªåŠ¨åŒæ­¥æŒ‡æ ‡
if self.is_distributed:
    avg_loss = reduce_tensor(avg_loss_tensor, op='mean')

# åªåœ¨ä¸»è¿›ç¨‹æ‰“å°å’Œä¿å­˜
if self.is_main:
    print(...)
    self.save_checkpoint(...)
```

---

## ğŸš€ æ€§èƒ½ä¼˜åŒ–

### 1. æ··åˆç²¾åº¦è®­ç»ƒ
- ä½¿ç”¨ `torch.amp.autocast` å’Œ `GradScaler`
- è‡ªåŠ¨åœ¨ CUDA ä¸Šå¯ç”¨
- å¯èŠ‚çœçº¦ 50% æ˜¾å­˜

### 2. æ¢¯åº¦ç´¯ç§¯
- æ¨¡æ‹Ÿæ›´å¤§çš„ batch size
- åœ¨æ˜¾å­˜æœ‰é™æ—¶å¾ˆæœ‰ç”¨
- æ¢¯åº¦æ­£ç¡®ç´¯ç§¯å¹¶å½’ä¸€åŒ–

### 3. æ¢¯åº¦è£å‰ª
- é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
- ä½¿ç”¨å…¨å±€èŒƒæ•°è£å‰ª
- é»˜è®¤é˜ˆå€¼ 1.0

---

## ğŸ“¦ ä¾èµ–å…³ç³»

```
BaseTrainer (åŸºç±»)
â”œâ”€â”€ utils.distributed (åˆ†å¸ƒå¼å·¥å…·)
â”œâ”€â”€ torch.amp (æ··åˆç²¾åº¦)
â””â”€â”€ torch.nn.parallel (DDP)

SFTTrainer (ç»§æ‰¿ BaseTrainer)
â””â”€â”€ torch.nn.functional (äº¤å‰ç†µ)

DPOTrainer (ç»§æ‰¿ BaseTrainer)
â”œâ”€â”€ å‚è€ƒæ¨¡å‹ (ref_model)
â””â”€â”€ å¯¹æ¯”æŸå¤±è®¡ç®—

GRPOTrainer (ç»§æ‰¿ BaseTrainer)
â”œâ”€â”€ å¥–åŠ±æ¨¡å‹ (å¯é€‰)
â”œâ”€â”€ åœ¨çº¿é‡‡æ ·é€»è¾‘
â””â”€â”€ PPO æŸå¤±è®¡ç®—
```

---

## âš¡ ä¸‹ä¸€æ­¥è®¡åˆ’

### çŸ­æœŸï¼ˆå¯é€‰ï¼‰
- [ ] æ·»åŠ  PPO è®­ç»ƒå™¨ï¼ˆå®Œæ•´ç‰ˆæœ¬ï¼Œå¸¦ criticï¼‰
- [ ] æ·»åŠ  ORPO è®­ç»ƒå™¨ï¼ˆOdds Ratio Preference Optimizationï¼‰
- [ ] æ·»åŠ è®­ç»ƒæŒ‡æ ‡å¯è§†åŒ–ï¼ˆTensorBoard/WandBï¼‰
- [ ] æ·»åŠ æ—©åœï¼ˆEarly Stoppingï¼‰åŠŸèƒ½

### ä¸­æœŸï¼ˆå¯é€‰ï¼‰
- [ ] æ”¯æŒ FSDPï¼ˆæ›´å¤§æ¨¡å‹ï¼‰
- [ ] æ”¯æŒæ¨¡å‹é‡åŒ–è®­ç»ƒ
- [ ] æ”¯æŒ LoRA/QLoRA å¾®è°ƒ
- [ ] æ·»åŠ æ›´å¤šå­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥

### é•¿æœŸï¼ˆå¯é€‰ï¼‰
- [ ] æ”¯æŒå¤šä»»åŠ¡å­¦ä¹ 
- [ ] æ”¯æŒè¯¾ç¨‹å­¦ä¹ ï¼ˆCurriculum Learningï¼‰
- [ ] æ·»åŠ æ›´å¤šçš„ RLHF å˜ä½“

---

## ğŸ“š å‚è€ƒèµ„æ–™

### è®ºæ–‡
- **SFT**: Standard supervised learning
- **DPO**: [Direct Preference Optimization](https://arxiv.org/abs/2305.18290)
- **PPO**: [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347)
- **GRPO**: Group-based PPO variant

### å®ç°å‚è€ƒ
- PyTorch DDP: https://pytorch.org/tutorials/intermediate/ddp_tutorial.html
- Transformers Trainer: https://github.com/huggingface/transformers
- TRL Library: https://github.com/huggingface/trl

---

## ğŸ“ å­¦ä¹ å»ºè®®

### å¯¹äºåˆå­¦è€…
1. é˜…è¯» `tutorial/trainer_from_scratch.py` ç†è§£è®­ç»ƒå¾ªç¯
2. æŸ¥çœ‹ `examples/` ä¸­çš„ç¤ºä¾‹ä»£ç 
3. ä½¿ç”¨ `SFTTrainer` è¿›è¡Œç®€å•çš„å¾®è°ƒå®éªŒ

### å¯¹äºè¿›é˜¶ç”¨æˆ·
1. é˜…è¯» `base_trainer.py` ç†è§£æ¶æ„è®¾è®¡
2. æ ¹æ®éœ€è¦ç»§æ‰¿ `BaseTrainer` å®ç°è‡ªå®šä¹‰è®­ç»ƒå™¨
3. ä½¿ç”¨ DPO æˆ– GRPO è¿›è¡Œé«˜çº§å¯¹é½è®­ç»ƒ

### å¯¹äºç ”ç©¶è€…
1. ç†è§£ä¸åŒè®­ç»ƒèŒƒå¼çš„æ•°å­¦åŸç†
2. å¯¹æ¯”ä¸åŒè®­ç»ƒå™¨çš„æ€§èƒ½å’Œæ•ˆæœ
3. åŸºäºç°æœ‰æ¡†æ¶å®ç°æ–°çš„è®­ç»ƒç®—æ³•

---

**æ€»ç»“**ï¼šæœ¬æ¬¡é‡æ„åˆ›å»ºäº†ä¸€ä¸ªæ¨¡å—åŒ–ã€å¯æ‰©å±•çš„è®­ç»ƒå™¨æ¶æ„ï¼Œæ”¯æŒä¸»æµçš„ LLM è®­ç»ƒèŒƒå¼ï¼ŒåŒæ—¶ä¿æŒä»£ç çš„æ¸…æ™°æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚æ‰€æœ‰è®­ç»ƒå™¨å…±äº«é€šç”¨é€»è¾‘ï¼Œåªéœ€å®ç°ä»»åŠ¡ç‰¹å®šçš„æ–¹æ³•ï¼Œå¤§å¤§æé«˜äº†ä»£ç å¤ç”¨ç‡ã€‚

**æœ€åæ›´æ–°**ï¼š2025-10-11  
**çŠ¶æ€**ï¼šâœ… å®Œæˆ

