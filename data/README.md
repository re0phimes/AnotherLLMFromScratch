# data/ 目录指导手册

> ⚠️ 此目录仅存放小规模「开发 / 调试」样本数据。凡是正式训练或需要大规模语料的任务，请务必通过 `configs/train/` 中的配置指向外部数据源，禁止直接把大数据集提交到仓库。

## 1. 数据量应该控制在什么范围？
- **调试阶段为何控制在 10⁴ ~ 10⁵ 条样本**：
  - 规模太大将显著拉长迭代时间，反而降低排错效率。
  - 这个量级足以覆盖常见边界情况，又能在本地 GPU / CPU 上分钟级跑通。
- **正式训练为何切换到外部数据源**：
  - 模型一旦超过 125M 参数，收敛速度高度依赖 Token 数量，通常需要 10⁸ 甚至 10⁹ 级别 Token。
  - 大规模数据通常跨多语言、多领域，体积以 TB 计，必须使用远程对象存储或 Hugging Face streaming 才能保证吞吐。
- **实践建议**：在配置里通过 `data.max_samples` 显式限制调试样本数；进入正式训练时，将该字段设为 `null` 并切换到 streaming 配置。

## 2. 多数据集如何平衡与处理？
- **为什么需要权重或分阶段训练**：不同语料的风格、语言、质量差异明显，直接混合会导致模型偏向高频数据源，影响下游效果。
- **推荐做法**：
  - 在 `configs/train/*.yaml` 中扩展 `data` 为 `data_sources` 数组，为每个数据集标注 `sampling_weight` 或 `target_ratio`。
  - Dataset 层按照权重进行 Token 级重采样，或在 Trainer 中实现“阶段性切换”（如先通用语料、再指令数据）。
  - 针对多语言任务，建议在配置中记录各语种样本比例，并在评估 prompts 中同步体现，确保训练/验证对齐。
- **必要的清洗与格式统一**：提前在外部脚本中完成去重、过滤敏感内容、字段归一化，导出统一的 `{"text": ...}` JSONL，再由本项目消费。

## 3. data 目录结构与示例
```
data/
├── raw_samples/           # 可选；存极小的原始片段样本
├── processed/             # 已清洗、格式统一的样本
└── README.md              # 本文档
```
- **raw_samples/**：
  - 仅保留字节级/文本级的小片段（如新闻段落、论文摘要），帮助理解原始噪声与格式。
  - 每个示例文件请注明来源与采集时间，体量保持在数 KB 内，并提醒后续需在外部脚本中清洗。
  - ⚠️ 不得存放敏感或完整原始数据，正式语料放在外部存储，通过配置引用。
- **processed/**：
  - 保存统一清洗后的 JSONL（`{"text": ...}`）调试样本，如 `news_sample.jsonl`、`pdf_sample.jsonl`、`paper_sample.jsonl`。
  - 若需附加元信息，可增加 `metadata` 字段，并在 README 内记录 schema，以免团队误用。
- **mix_manifest.yaml（可选）**：
  - 若项目需要说明默认配比或清洗策略，可在仓库以注释形式提供示例，便于沟通。

## 4. 本地多来源（新闻、PDF、论文等）如何标准化？
- **统一处理的重要性**：不同来源的文本编码、段落结构、噪声水平差异极大，如果不做归一化，模型会学习到无效 token 或格式。
- **建议工作流**：
  1. 在外部 `scripts/` 或独立工具中，针对每种来源编写清洗脚本（例如 PDF → 文本、新闻去广告、论文提取摘要）。
  2. 将结果统一导出为 `processed/<source_name>_sample.jsonl`，每行一个 JSON，对齐键名（至少包含 `text`，必要时增加 `metadata`）。
  3. 在 `mix_manifest.yaml` 或配置备注中记录：来源名称、原始体量、清洗规则、收益/风险评估。
  4. 通过训练配置里的 `data.data_sources` 声明每个来源的采样权重。例如：
     ```yaml
     data:
       type: "local"
       data_sources:
         - path: "processed/news_sample.jsonl"
           sampling_weight: 0.5
         - path: "processed/paper_sample.jsonl"
           sampling_weight: 0.3
         - path: "processed/pdf_sample.jsonl"
           sampling_weight: 0.2
     ```
  5. Dataset 模块读取该结构后，负责混合与随机化，保证训练时能按比例取样。
- **比例选择依据**：结合业务需求（如偏新闻、偏学术）、语料质量（清洗程度、正确率）以及目标语言覆盖率。必要时先在验证集上实验不同配比再确定最终参数。

## 5. 正式训练数据的存放策略
- **远程 streaming**：在 YAML 中写明 `type: "streaming"`、`provider: "huggingface"`、`path`、`subset` 等参数，Dataset 会调用 `datasets` 库按需迭代。
- **本地 NAS / 对象存储**：在配置中提供绝对路径（如 `/mnt/data/my_corpus.jsonl`），不要写死到代码里，便于不同环境复用。
- **脚本管理**：清洗/下载脚本放在 `scripts/` 或独立仓库，避免把中间步骤产物堆积在 Git 里。

## 6. 其他注意事项
- **可复现性**：记录所有调试语料的 SHA256、来源链接和获取时间；对 streaming 数据注明版本标签或快照 ID。
- **合规与安全**：确保数据具备合法授权，上传前检查是否包含个人隐私或敏感信息；若需要认证令牌（如 `${HF_TOKEN}`），只在配置中引用环境变量，不直接写死。
- **格式标准化**：项目默认输入结构为一行一个样本的 JSONL，键名为 `text`；若未来扩展多字段，可在 dataset 模块集中处理，避免各脚本重复解析逻辑。
- **抽样检查**：引入新数据源前，先抽样（如 1k 条）进行人工检查，确认语言比例、文本质量、去重情况符合预期。
- **团队协作**：为不同语料准备独立说明（来源、过滤规则、预处理脚本），并在 YAML 中引用这些文档，帮助新成员快速理解数据管线。
